---
title: "Practical Machine Language"
author: '@drktjones'
date: "October 25, 2015"
output: html_document
---

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible 
to collect a large amount of data about personal activity relatively inexpensively. 
These type of devices are part of the quantified self movement â€“ a group of 
enthusiasts who take measurements about themselves regularly to improve their 
health, to find patterns in their behavior, or because they are tech geeks. 
One thing that people regularly do is quantify how much of a particular activity 
they do, but they rarely quantify how well they do it. 

In this project, my goal will be to use data from accelerometers on the belt, 
forearm, arm, and dumbell of 6 participants. They were asked to perform barbell 
lifts correctly and incorrectly in 5 different ways. More information is available 
from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on 
the Weight Lifting Exercise Dataset). 

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:         
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The goal of this project is to predict the manner in which they did the 
exercise using the "classe" variable in the training set. Details describing 
how I built the model, how I used cross validation, what I think the 
expected out of sample error is, and why I made the choices I did. I also use 
your prediction model to predict 20 different test cases. 

# Model Building

My first step was to load the data, examine the data, and load relevant libraries 
used in these analyses. 

```{r}

library(caret)
library(rpart)
library(rpart.plot)
library(ggplot2)

set.seed(32343)

dat_train <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),
                      na.strings=c("", "NA"))
dat_test <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), 
                     na.strings=c("", "NA"))

dim(dat_train)
dim(dat_test)
```

Next, I removed variables that had one distinct value or near zero variance in
order to obtain a better prediction. I also removed variables with missing data.
I decided against imputing missing variables. Finally, I removed the first 6
columns of the data frames because I did not want to use these in the prediction
given the structure of these data.

```{r}
nsv <- nearZeroVar(dat_train, saveMetrics = TRUE)
head(nsv)

dat_train1 <- dat_train[names(dat_train) %in% 
                                 row.names(nsv[nsv$zeroVar==FALSE & nsv$nzv==FALSE,])]
dat_test1 <- dat_test[names(dat_test) %in% 
                               c(row.names(nsv[nsv$zeroVar==FALSE & nsv$nzv==FALSE,]),
                                 "problem_id")]

ncol(dat_train1); ncol(dat_test1)
names(dat_train1)

trainmiss <- apply(dat_train1,2,function(x) {sum(is.na(x))})
dat_train2 <- dat_train1[,which(trainmiss == 0)]

testmiss <- apply(dat_test1,2,function(x) {sum(is.na(x))})
dat_test2 <- dat_test1[,which(testmiss == 0)]
ncol(dat_train2); ncol(dat_test2)
names(dat_train2)

dat_train2 <- dat_train2[, -c(1:6)]
dat_test2 <- dat_test2[, -c(1:6)]
```


# Cross Validation and Expected Out Error

Next I created a training and testing data set and examined their dimensions. 
Finally, I plot the outcome variable to get a sense of its distribution.

```{r}
inTrain <- createDataPartition(y = dat_train2$classe, p=0.75, list = FALSE)
training <- dat_train2[inTrain, ]
testing <- dat_train2[-inTrain, ]
dim(training)
dim(testing)

plot(training$classe)
```

# Prediction

I used two different predictions to test for accuracy: linear discriminant 
analysis (LDA) and classification tree (CT). These models were decided based on the 
non-linear nature of the data. 

## LDA
Below I created the LDA prediction model and obtain the accuracy of the prediction. 

```{r}
modlda = train(classe ~ ., data = training, method = "lda")
plda = predict(modlda, testing)
confusionMatrix(plda, testing$classe)
```

The accuracy of the LDA model is 0.69 (95% CI = 0.68 - 0.71), which means the 
expected sample out erroris 0.31. 

Below I create and plot the CT prediction model and plot the classification tree. 

```{r}

modct <- rpart(classe ~ ., data=training, method="class")
rpart.plot(modct, main="Classification Tree", under=TRUE, faclen=0)
```

Next I obtain the accuracy of the CT prediction model.

```{r}
pct <- predict(modct, testing, type = "class")
confusionMatrix(pct, testing$classe)
```

The accuracy of the RF model is 0.72 (95% CI = 0.71 - 0.73), which means the 
expected sample out error is 0.28.

# Prediction 

Based on the overlapping confidence intervals from the confusionMatrix output
for both models, the accuracy of the models are similar. Therefore, the LDA model
is used for the final predictions for the assignment test data set.

```{r}
predicttest <- predict(modlda, dat_test2)
predicttest

pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                files = paste0("problem_id_",i,".txt")
                write.table(x[i],file=files,col.names=FALSE,row.names=FALSE,quote=FALSE)
        }
}

pml_write_files(predicttest)
```







